{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Redis for Caching\n",
    "\n",
    "This notebook demonstrates how to use the `RedisCache` and `RedisSemanticCache` classes from the langchain-redis package to implement caching for LLM responses using Redis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Installation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install ipywidgets\n",
    "%pip install langchain-core\n",
    "%pip install langchain-redis\n",
    "%pip install langchain-openai\n",
    "%pip install redis"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Importing Required Libraries"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import redis\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_redis import RedisCache, RedisSemanticCache"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting up Redis Connection"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "REDIS_URL = \"redis://localhost:6379\"\n",
    "redis_client = redis.from_url(REDIS_URL)\n",
    "redis_client.ping()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Set the OpenAI API key"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "openai_api_key = os.getenv(\"OPENAI_API_KEY\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Using Redis as a Standard Cache"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "redis_cache = RedisCache(redis_url=REDIS_URL)\n",
    "set_llm_cache(redis_cache)\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "def execute_with_timing(prompt):\n",
    "    start_time = time.time()\n",
    "    result = llm.invoke(prompt)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "# First call (not cached)\n",
    "prompt = \"Explain the concept of caching in three sentences.\"\n",
    "result1, time1 = execute_with_timing(prompt)\n",
    "print(f\"First call (not cached):\")\n",
    "print(f\"{result1}\\nTime: {time1:.2f} seconds\\n\")\n",
    "\n",
    "# Second call (should be cached)\n",
    "result2, time2 = execute_with_timing(prompt)\n",
    "print(f\"Second call (cached):\")\n",
    "print(f\"{result2}\\nTime: {time2:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time2:.2f}x faster\")\n",
    "\n",
    "# Clear the cache\n",
    "redis_cache.clear()\n",
    "print(\"Cache cleared\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Using Redis as a Semantic Cache"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "semantic_cache = RedisSemanticCache(\n",
    "    redis_url=REDIS_URL, embeddings=embeddings, distance_threshold=0.2\n",
    ")\n",
    "\n",
    "set_llm_cache(semantic_cache)\n",
    "\n",
    "# Original prompt\n",
    "original_prompt = \"What is the capital of France?\"\n",
    "result1, time1 = execute_with_timing(original_prompt)\n",
    "print(f\"Original query:\\nPrompt: {original_prompt}\\n\")\n",
    "print(f\"{result1}\\nTime: {time1:.2f} seconds\\n\")\n",
    "\n",
    "# Semantically similar prompt\n",
    "similar_prompt = \"Can you tell me the capital city of France?\"\n",
    "result2, time2 = execute_with_timing(similar_prompt)\n",
    "print(f\"Similar query:\\nPrompt: {similar_prompt}\\n\")\n",
    "print(f\"{result2}\\nTime: {time2:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time2:.2f}x faster\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cleanup"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Clear the semantic cache\n",
    "semantic_cache.clear()\n",
    "print(\"Semantic cache cleared\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
